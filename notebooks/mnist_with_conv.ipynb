{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from edunets import functional, losses\n",
    "from edunets.tensor import Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "### Making sure the basic convolution operation (or cross-correlation) works\n",
    "\n",
    "Before anything else, let's make sure edunet's can perform simple matrix convolutions.\n",
    "\n",
    "Let's start by making sure the kernel I = [[0, 0, 0], [0, 1, 0], [0, 0, 0]], is the 'identity' kernel for convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch input (shape=torch.Size([1, 1, 3, 4])):\n",
      "tensor([[[[1., 2., 3., 4.],\n",
      "          [1., 2., 3., 4.],\n",
      "          [1., 2., 3., 4.]]]], dtype=torch.float64, requires_grad=True)\n",
      "edunets input:\n",
      "tensor([[[[1., 2., 3., 4.],\n",
      "         [1., 2., 3., 4.],\n",
      "         [1., 2., 3., 4.]]]], requires_grad=True)\n",
      "torch kernel (shape=torch.Size([1, 1, 3, 3])):\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]]]], dtype=torch.float64)\n",
      "edunets kernel:\n",
      "tensor([[[[0., 0., 0.],\n",
      "         [0., 1., 0.],\n",
      "         [0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "torch_input = torch.tensor(np.array([[1.,2.,3.,4.], [1.,2.,3.,4.], [1.,2.,3.,4.]])[None, None], requires_grad=True)\n",
    "edu_input = Tensor(np.array([[1., 2., 3., 4.], [1.,2.,3.,4.], [1.,2.,3.,4.]])[None, None], requires_grad=True)\n",
    "torch_kernel = torch.tensor(np.array([[0., 0., 0.], [0., 1., 0.], [0., 0., 0.]])[None, None])\n",
    "edu_kernel = Tensor(np.array([[0., 0., 0.], [0., 1., 0.], [0., 0., 0.]])[None, None])\n",
    "\n",
    "print(f\"torch input (shape={torch_input.shape}):\")\n",
    "print(torch_input)\n",
    "print(\"edunets input:\")\n",
    "print(edu_input)\n",
    "print(f\"torch kernel (shape={torch_kernel.shape}):\")\n",
    "print(torch_kernel)\n",
    "print(\"edunets kernel:\")\n",
    "print(edu_kernel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test if the kernel is the identity for convolutions, we need to pad the input first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 4]) -> torch.Size([1, 1, 5, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 2., 3., 4.],\n",
       "          [1., 2., 3., 4.],\n",
       "          [1., 2., 3., 4.]]]], dtype=torch.float64,\n",
       "       grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_padded_input = F.pad(torch_input, (1, 1, 1, 1))\n",
    "print(torch_input.shape, \"->\", torch_padded_input.shape)\n",
    "F.conv2d(torch_padded_input, torch_kernel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use the padding=\"same\" directive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2., 3., 4.],\n",
      "          [1., 2., 3., 4.],\n",
      "          [1., 2., 3., 4.]]]], dtype=torch.float64,\n",
      "       grad_fn=<ThnnConv2DBackward>)\n",
      "Gradient of input:\n",
      "tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Convolution using PyTorch\n",
    "out = F.conv2d(torch_input, torch_kernel, padding=\"same\")\n",
    "\n",
    "print(out)\n",
    "\n",
    "out.backward(torch.ones(out.shape))\n",
    "\n",
    "print(\"Gradient of input:\")\n",
    "print(torch_input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2., 3., 4.],\n",
      "         [1., 2., 3., 4.],\n",
      "         [1., 2., 3., 4.]]]], dtype=float32)\n",
      "Gradient of input:\n",
      "[[[[1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1.]]]]\n"
     ]
    }
   ],
   "source": [
    "# Convolution using Edunets\n",
    "out = functional.conv(edu_input, edu_kernel, padding=\"same\")\n",
    "\n",
    "print(out)\n",
    "\n",
    "out.backward(Tensor.ones(*out.shape))\n",
    "\n",
    "print(\"Gradient of input:\")\n",
    "print(edu_input.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Our kernel acts like the identity for convolutions in both frameworks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's add channels and batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f448d7997c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An image is encoded as channels x height x width\n",
    "# Here is a 3x3 red image:\n",
    "dummy_red_img = np.array([\n",
    "    [\n",
    "        [255., 255., 255.],\n",
    "        [255., 255., 255.],\n",
    "        [255., 255., 255.]\n",
    "    ],\n",
    "    [\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.]\n",
    "    ],\n",
    "    [\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.]\n",
    "    ]\n",
    "])\n",
    "\n",
    "plt.imshow(dummy_red_img.transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f448d69a5e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_blue_img = np.array([\n",
    "    [\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.]\n",
    "    ],\n",
    "    [\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.]\n",
    "    ],\n",
    "    [\n",
    "        [255., 255., 255.],\n",
    "        [255., 255., 255.],\n",
    "        [255., 255., 255.]\n",
    "    ]\n",
    "])\n",
    "\n",
    "plt.imshow(dummy_blue_img.transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[255., 255., 255.],\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.]],\n",
       "\n",
       "         [[  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.]],\n",
       "\n",
       "         [[  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "        [[[  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.]],\n",
       "\n",
       "         [[  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.]],\n",
       "\n",
       "         [[255., 255., 255.],\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.]]]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_input = torch.tensor([dummy_red_img, dummy_blue_img], requires_grad=True)\n",
    "torch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the kernel: torch.Size([1, 3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_kernel = torch.ones((3,3), dtype=torch.float64).repeat(1, 3, 1, 1)\n",
    "print(\"Shape of the kernel:\", torch_kernel.shape)\n",
    "torch_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2295.]]],\n",
       "\n",
       "\n",
       "        [[[2295.]]]], dtype=torch.float64, grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(torch_input, torch_kernel) # note: 2295 = 255 * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2295.]]],\n",
       "\n",
       "\n",
       "       [[[2295.]]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now in edunets\n",
    "edu_input = Tensor(torch_input.detach().numpy())\n",
    "edu_kernel = Tensor(torch_kernel.detach().numpy())\n",
    "functional.conv(edu_input, edu_kernel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Conv2d in edunets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride=1, padding=0, bias=True):\n",
    "        # Paramaters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initiating Kernel\n",
    "        self.real_kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "        stdv = 1./np.sqrt(math.prod(self.real_kernel_size))\n",
    "        self.kernel = Tensor.uniform(*self.real_kernel_size, low=-stdv, high=stdv, requires_grad=True)\n",
    "\n",
    "        # Initiating Bias\n",
    "        if bias is True:\n",
    "            stdv = 1./np.sqrt(out_channels if isinstance(out_channels, int) else math.prod(out_channels))\n",
    "            self.bias = Tensor.uniform(1, out_channels, 1, 1, low=-stdv, high=stdv, requires_grad=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        batch, in_channels, width, height = x.shape\n",
    "\n",
    "        # Reshaping kernel and bias to match the channels and depth\n",
    "        self.kernel = self.kernel.repeat((batch, in_channels, 1, 1))\n",
    "\n",
    "        convolution = functional.conv(x, self.kernel, stride=self.stride, padding=self.padding)\n",
    "        print(convolution.shape)\n",
    "        convolution = convolution.repeat((1, self.out_channels, 1, 1))\n",
    "        \n",
    "        if self.bias:\n",
    "            _, _, new_width, new_height = convolution.shape\n",
    "            return self.bias.repeat((batch, 1, new_width, new_height)) + convolution\n",
    "        return convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 2, 2]), tensor([[[[-0.0819, -0.0819],\n",
       "           [-0.0819, -0.0819]],\n",
       " \n",
       "          [[-0.0844, -0.0844],\n",
       "           [-0.0844, -0.0844]],\n",
       " \n",
       "          [[ 0.2836,  0.2836],\n",
       "           [ 0.2836,  0.2836]],\n",
       " \n",
       "          [[-0.0323, -0.0323],\n",
       "           [-0.0323, -0.0323]],\n",
       " \n",
       "          [[-0.3218, -0.3218],\n",
       "           [-0.3218, -0.3218]],\n",
       " \n",
       "          [[ 0.1167,  0.1167],\n",
       "           [ 0.1167,  0.1167]]]], grad_fn=<ThnnConv2DBackward>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple pytorch example\n",
    "\n",
    "# 3 (input_channels) 6 (out_channels) 3 (kernel_size)\n",
    "m = nn.Conv2d(3, 6, 3, bias=False)\n",
    "# 1 (batch size) 2 (input_channels) 4 (height) 4 (width)\n",
    "input = torch.ones(1, 3, 4, 4)\n",
    "output = m(input)\n",
    "# 1 (batch size) 6 (out_channels) 2 (height post kernel) 2 (width post kernel)\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1, 6, 2, 2), tensor([[[[2.644187, 2.644187],\n",
       "          [2.644187, 2.644187]],\n",
       " \n",
       "         [[2.644187, 2.644187],\n",
       "          [2.644187, 2.644187]],\n",
       " \n",
       "         [[2.644187, 2.644187],\n",
       "          [2.644187, 2.644187]],\n",
       " \n",
       "         [[2.644187, 2.644187],\n",
       "          [2.644187, 2.644187]],\n",
       " \n",
       "         [[2.644187, 2.644187],\n",
       "          [2.644187, 2.644187]],\n",
       " \n",
       "         [[2.644187, 2.644187],\n",
       "          [2.644187, 2.644187]]]], dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edunets equivalent of pytorch's simple example\n",
    "\n",
    "# 3 (input_channels) 6 (out_channels) 3 (kernel_size)\n",
    "m = Conv2d(3, 6, 3, bias=False)\n",
    "# 1 (batch size) 2 (input_channels) 4 (height) 4 (width)\n",
    "input = Tensor.ones(1, 3, 4, 4)\n",
    "output = m(input)\n",
    "# 1 (batch size) 6 (out_channels) 2 (height post kernel) 2 (width post kernel)\n",
    "output.shape, output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST in Edunets (with convolutional layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7fc7891fccd0>, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the mnist dataset\n",
    "\n",
    "def fetch(url):\n",
    "    import requests, gzip, os, hashlib, numpy\n",
    "    fp = os.path.join(\"/tmp\", hashlib.md5(url.encode('utf-8')).hexdigest())\n",
    "    if os.path.isfile(fp):\n",
    "        with open(fp, \"rb\") as f:\n",
    "            dat = f.read()\n",
    "    else:\n",
    "        with open(fp, \"wb\") as f:\n",
    "            dat = requests.get(url).content\n",
    "            f.write(dat)\n",
    "    return numpy.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy()\n",
    "\n",
    "X_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[0x10:].reshape((-1, 28, 28))\n",
    "Y_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:]\n",
    "X_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[0x10:].reshape((-1, 28, 28))\n",
    "Y_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:]\n",
    "\n",
    "plt.imshow(X_train[0]), Y_train[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 12 2021, 13:49:34) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7dbd43346cf4060a7c5249e62daf2cb2395365dab3b14076ef7b34680f245741"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
